{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.Installing required packages and libraries**"
      ],
      "metadata": {
        "id": "Xq9jRg1FndzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYpmmfQ-ZbIh"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade langchain openai  -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
      ],
      "metadata": {
        "id": "5D5W_BeBZf5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured -q\n",
        "!pip install unstructured[local-inference] -q\n",
        "!pip install detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2 -q"
      ],
      "metadata": {
        "id": "XbL5zdf9aG8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils  "
      ],
      "metadata": {
        "id": "HwalYTVZoRlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Loading documents**\n",
        "\n",
        "First, we need to load the documents from a directory using the DirectoryLoader from LangChain. In this example, we assume the documents are stored in a directory called 'data'.\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/directory_loader.html"
      ],
      "metadata": {
        "id": "TItvBYkXnzod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "directory = '/content/data'\n",
        "\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "fulYnj9nZr3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Splitting documents**\n",
        "\n",
        "Now, we need to split the documents into smaller chunks for processing. We will use the RecursiveCharacterTextSplitter from LangChain, which by default tries to split on the characters [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
        "\n",
        "https://python.langchain.com/en/latest/modules/indexes/text_splitters/getting_started.html\n",
        "\n"
      ],
      "metadata": {
        "id": "zKz-Qs43oDCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "5lF8jA6xZ0Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Embedding documents with OpenAI**\n",
        "\n",
        "Once the documents are split, we need to embed them using OpenAI's language model. First, we need to install the tiktoken library."
      ],
      "metadata": {
        "id": "CTAY0YP1ooSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#requires for open ai embedding\n",
        "!pip install tiktoken -q"
      ],
      "metadata": {
        "id": "6PiPwt-FaYwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can use the OpenAIEmbeddings class from LangChain to embed the documents."
      ],
      "metadata": {
        "id": "1dW35uQmor9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "\n",
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "len(query_result)"
      ],
      "metadata": {
        "id": "F5GY9voPa0av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Vector search with Pinecone**\n",
        "\n",
        "Next, we will use Pinecone to create an index for our documents. First, we need to install the pinecone-client."
      ],
      "metadata": {
        "id": "1g2667QWo2Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client -q"
      ],
      "metadata": {
        "id": "LXhIY5SrrRec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can initialize Pinecone and create a Pinecone index.The Pinecone.from_documents() method creates a new Pinecone vector index using docs, embeddings, and index_name arguments. Docs are a list of smaller document chunks, embeddings convert text to numerical representations, and index_name is a unique identifier for the index. The method generates embeddings, indexes them, and creates an index object that can perform similarity searches and retrieve relevant documents.\n",
        "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html"
      ],
      "metadata": {
        "id": "vySq5oI5sU5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone \n",
        "from langchain.vectorstores import Pinecone\n",
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=\"pinecone api key\",  # find at app.pinecone.io\n",
        "    environment=\"env\"  # next to api key in console\n",
        ")\n",
        "\n",
        "index_name = \"langchain-demo\"\n",
        "\n",
        "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "hfIpYLV-acks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Finding similar documents**\n",
        "\n",
        "Now, we can define a function to find similar documents based on a given query."
      ],
      "metadata": {
        "id": "0o2QuwEspd0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similiar_docs(query,k=2,score=False):\n",
        "  if score:\n",
        "    similar_docs = index.similarity_search_with_score(query,k=k)\n",
        "  else:\n",
        "    similar_docs = index.similarity_search(query,k=k)\n",
        "  return similar_docs\n",
        "\n",
        "query = \"How is india's economy\"\n",
        "similar_docs = get_similiar_docs(query)\n",
        "similar_docs"
      ],
      "metadata": {
        "id": "o5r7YLpbchAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Question answering using LangChain and OpenAI LLM**\n",
        "\n",
        "With the necessary components in place, we can now create a question-answering system using the OpenAI class from LangChain and a pre-built question-answering chain."
      ],
      "metadata": {
        "id": "rtF9QZSvbLD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# model_name = \"text-davinci-003\"\n",
        "# model_name = \"gpt-3.5-turbo\"\n",
        "model_name = \"gpt-4\"\n",
        "llm = OpenAI(model_name=model_name)"
      ],
      "metadata": {
        "id": "DuevPx4dbI4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Example queries and answers**\n",
        "Finally, let's test our question answering system with some example queries.\n",
        "https://python.langchain.com/en/latest/use_cases/question_answering.html"
      ],
      "metadata": {
        "id": "lvqmkpJvss17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "def get_answer(query):\n",
        "  similar_docs = get_similiar_docs(query)\n",
        "  # print(similar_docs)\n",
        "  answer =  chain.run(input_documents=similar_docs, question=query)\n",
        "  return  answer\n",
        "\n",
        "query = \"Your_query\"  \n",
        "get_answer(query)"
      ],
      "metadata": {
        "id": "RqCE-C3Ubty0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Your_query\"\n",
        "get_answer(query)"
      ],
      "metadata": {
        "id": "RTnCqRJ3r1kf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}